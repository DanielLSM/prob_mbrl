{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from kusanagi.shell import cartpole\n",
    "from kusanagi.base import ExperienceDataset, apply_controller\n",
    "from kusanagi.ghost.control import RandPolicy\n",
    "\n",
    "from prob_mbrl import utils, models, algorithms, losses, train_regressor\n",
    "torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(states, actions, dynamics, **kwargs):\n",
    "    deltas, rewards = dyn((states, actions), return_samples=True,\n",
    "                           separate_outputs=True, **kwargs)\n",
    "    next_states = states + deltas\n",
    "    return next_states, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-07-11 19:15:51.740464] Experience > Initialising new experience dataset\n"
     ]
    }
   ],
   "source": [
    "dyn_components = 4\n",
    "dyn_layers = 2\n",
    "pol_layers = 2\n",
    "\n",
    "env = cartpole.Cartpole()\n",
    "target = torch.tensor([0,0,0,np.pi]).float()\n",
    "D = target.shape[-1]\n",
    "U = 1\n",
    "learn_reward = False\n",
    "maxU = np.array([10.0])     \n",
    "angle_dims = torch.tensor([3]).long()\n",
    "target = utils.to_complex(target, angle_dims)\n",
    "Da = target.shape[-1]\n",
    "Q = torch.zeros(Da, Da).float()\n",
    "Q[0, 0] = 1\n",
    "Q[0, -2] = env.l\n",
    "Q[-2, 0] = env.l\n",
    "Q[-2, -2] = env.l**2\n",
    "Q[-1, -1] = env.l**2\n",
    "Q /= 0.1\n",
    "def reward_func(states, target, Q, angle_dims):\n",
    "    states = utils.to_complex(states, angle_dims)\n",
    "    reward = losses.quadratic_saturating_loss(states, target, Q)\n",
    "    return reward\n",
    "\n",
    "dynE = 2*(D+1) if learn_reward else 2*D\n",
    "reward_func = None if learn_reward else partial(reward_func, target=target, Q=Q, angle_dims=angle_dims)\n",
    "dyn = models.DynamicsModel(\n",
    "        models.dropout_mlp(\n",
    "            Da+U, (dynE+1)*dyn_components, [200]*dyn_layers,\n",
    "            nonlin=torch.nn.ReLU,\n",
    "            dropout_layers=[models.modules.CDropout(0.5, 0.1)]*dyn_layers\n",
    "        ),\n",
    "        reward_func=reward_func, angle_dims=angle_dims,\n",
    "        output_density=models.MixtureDensity(dynE/2, dyn_components)\n",
    "    ).float()\n",
    "\n",
    "pol = models.Policy(\n",
    "    models.dropout_mlp(\n",
    "        Da, U, [200]*pol_layers,\n",
    "        nonlin=torch.nn.ReLU,\n",
    "        output_nonlin=torch.nn.Tanh,\n",
    "        dropout_layers=[models.modules.BDropout(0.5)]*pol_layers),\n",
    "    maxU, angle_dims=angle_dims).float()\n",
    "randpol = RandPolicy(maxU)\n",
    "exp = ExperienceDataset()\n",
    "params = filter(lambda p: p.requires_grad, pol.parameters())\n",
    "opt = torch.optim.Adam(params, 1e-3, amsgrad=True)\n",
    "\n",
    "forward_fn = partial(forward, dynamics=dyn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-07-11 19:15:51.758118] apply_controller > Starting run\n",
      "[2018-07-11 19:15:51.759424] apply_controller > Running for 2.500000 seconds\n",
      "[2018-07-11 19:15:51.902988] apply_controller > Done. Stopping robot. Value of run [24.976482]\n",
      "[2018-07-11 19:15:51.904354] Cartpole > Stopping robot\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib qt\n",
    "def cb(*args, **kwargs):\n",
    "    env.render()\n",
    "\n",
    "H = 25\n",
    "N_particles = 100\n",
    "for rand_it in range(1):\n",
    "    ret = apply_controller(env, randpol, H, callback=None)\n",
    "    exp.append_episode(*ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-07-11 19:15:52.006581] apply_controller > Starting run\n",
      "[2018-07-11 19:15:52.008157] apply_controller > Running for 2.500000 seconds\n",
      "[2018-07-11 19:15:52.137672] apply_controller > Done. Stopping robot. Value of run [24.993557]\n",
      "[2018-07-11 19:15:52.138901] Cartpole > Stopping robot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 0.126448:  37%|███▋      | 1835/5000 [00:14<00:24, 129.67it/s] "
     ]
    }
   ],
   "source": [
    "for ps_it in range(100):\n",
    "    # apply policy\n",
    "    ret = apply_controller(env, pol, H, callback=None)\n",
    "    exp.append_episode(*ret)\n",
    "\n",
    "    # train dynamics\n",
    "    X, Y = exp.get_dynmodel_dataset(deltas=True, return_costs=learn_reward)\n",
    "    dyn.set_dataset(torch.tensor(X).to(dyn.X.device).float(), torch.tensor(Y).to(dyn.X.device).float())  \n",
    "    train_regressor(dyn, 5000, N_particles, True, log_likelihood=losses.gaussian_mixture_log_likelihood)\n",
    "    x0 = torch.tensor(exp.sample_states(N_particles, timestep=0)).to(dyn.X.device).float()\n",
    "    x0 += 1e-2*x0.std(0)*torch.randn_like(x0)\n",
    "    utils.plot_rollout(x0, forward_fn, pol, H)\n",
    "    \n",
    "    # train policy\n",
    "    print \"Policy search iteration %d\" % (ps_it+1)\n",
    "    algorithms.mc_pilco(x0, forward_fn, dyn, pol, H, opt, opt_iters=1000, exp=exp,\n",
    "             maximize=False, pegasus=False, mm_states=False,\n",
    "             mm_rewards=False, mpc=False, max_steps=25)\n",
    "    utils.plot_rollout(x0, forward_fn, pol, H)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
