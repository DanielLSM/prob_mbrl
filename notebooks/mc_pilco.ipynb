{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from kusanagi.shell import cartpole\n",
    "from kusanagi.base import ExperienceDataset, apply_controller\n",
    "from kusanagi.ghost.control import RandPolicy\n",
    "\n",
    "from prob_mbrl import utils, models, algorithms, losses, train_regressor\n",
    "torch.set_flush_denormal(True)\n",
    "torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(states, actions, dynamics, **kwargs):\n",
    "    deltas, rewards = dynamics(\n",
    "        (states, actions), return_samples=True,\n",
    "        separate_outputs=True, **kwargs)\n",
    "    next_states = states + deltas\n",
    "    return next_states, rewards\n",
    "\n",
    "\n",
    "def reward_fn(states, target, Q, angle_dims):\n",
    "    states = utils.to_complex(states, angle_dims)\n",
    "    reward = losses.quadratic_saturating_loss(states, target, Q)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "H = 25\n",
    "N_particles = 100\n",
    "dyn_components = 4\n",
    "dyn_hidden = [200]*2\n",
    "pol_hidden = [200]*2\n",
    "use_cuda = False\n",
    "\n",
    "# initialize environment\n",
    "env = cartpole.Cartpole()\n",
    "\n",
    "# initialize reward/cost function\n",
    "target = torch.tensor([0, 0, 0, np.pi]).float()\n",
    "D = target.shape[-1]\n",
    "U = 1\n",
    "learn_reward = False\n",
    "maxU = np.array([10.0])\n",
    "angle_dims = torch.tensor([3]).long()\n",
    "target = utils.to_complex(target, angle_dims)\n",
    "Da = target.shape[-1]\n",
    "Q = torch.zeros(Da, Da).float()\n",
    "Q[0, 0] = 1\n",
    "Q[0, -2] = env.l\n",
    "Q[-2, 0] = env.l\n",
    "Q[-2, -2] = env.l**2\n",
    "Q[-1, -1] = env.l**2\n",
    "Q /= 0.1\n",
    "if learn_reward:\n",
    "    reward_func = None\n",
    "else:\n",
    "    reward_func = partial(\n",
    "        reward_fn, target=target, Q=Q, angle_dims=angle_dims)\n",
    "\n",
    "# initialize dynamics model\n",
    "dynE = 2*(D+1) if learn_reward else 2*D\n",
    "dyn_model = models.dropout_mlp(\n",
    "            Da+U, (dynE+1)*dyn_components, dyn_hidden,\n",
    "            dropout_layers=[models.modules.CDropout(0.5, 0.1)\n",
    "                            for i in range(len(dyn_hidden))],\n",
    "            nonlin=torch.nn.ReLU,\n",
    "            weights_initializer=torch.nn.init.xavier_normal_,\n",
    "            biases_initializer=partial(torch.nn.init.uniform_, a=-1.0, b=1.0),\n",
    "        )\n",
    "dyn = models.DynamicsModel(\n",
    "    dyn_model, reward_func=reward_func,\n",
    "    angle_dims=angle_dims,\n",
    "    output_density=models.MixtureDensity(dynE/2, dyn_components)).float()\n",
    "\n",
    "# initalize policy\n",
    "pol_model = models.dropout_mlp(\n",
    "        Da, U, pol_hidden,\n",
    "        dropout_layers=[models.modules.BDropout(0.1)\n",
    "                        for i in range(len(pol_hidden))],\n",
    "        nonlin=torch.nn.ReLU,\n",
    "        output_nonlin=torch.nn.Tanh)\n",
    "\n",
    "pol = models.Policy(pol_model, maxU, angle_dims=angle_dims).float()\n",
    "randpol = RandPolicy(maxU)\n",
    "\n",
    "# initalize experience dataset\n",
    "exp = ExperienceDataset()\n",
    "\n",
    "# initialize dynamics optimizer\n",
    "opt1 = torch.optim.Adam(dyn.parameters(), 1e-3, amsgrad=True)\n",
    "\n",
    "# initialize policy optimizer\n",
    "opt2 = torch.optim.Adam(pol.parameters(), 1e-3, amsgrad=True)\n",
    "\n",
    "# define functions required for rollouts\n",
    "forward_fn = partial(forward, dynamics=dyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collect initial random experience\n",
    "for rand_it in range(1):\n",
    "    ret = apply_controller(\n",
    "        env, randpol, H,\n",
    "        callback=None)\n",
    "    exp.append_episode(*ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_cuda and torch.cuda.is_available():\n",
    "    dyn = dyn.cuda()\n",
    "    pol = pol.cuda()\n",
    "\n",
    "# policy learning loop\n",
    "for ps_it in range(100):\n",
    "    # apply policy\n",
    "    ret = apply_controller(\n",
    "        env, pol, H,\n",
    "        callback=None)\n",
    "    exp.append_episode(*ret)\n",
    "\n",
    "    # train dynamics\n",
    "    X, Y = exp.get_dynmodel_dataset(deltas=True, return_costs=learn_reward)\n",
    "    dyn.set_dataset(\n",
    "        torch.tensor(X).to(dyn.X.device).float(),\n",
    "        torch.tensor(Y).to(dyn.X.device).float())\n",
    "    train_regressor(\n",
    "        dyn, 2000, N_particles, True, opt1,\n",
    "        log_likelihood=losses.gaussian_mixture_log_likelihood)\n",
    "\n",
    "    # sample initial states for policy optimization\n",
    "    x0 = torch.tensor(\n",
    "        exp.sample_states(N_particles, timestep=0)).to(dyn.X.device).float()\n",
    "    x0 += 1e-2*x0.std(0)*torch.randn_like(x0)\n",
    "    utils.plot_rollout(x0, forward_fn, pol, H)\n",
    "\n",
    "    # train policy\n",
    "    print \"Policy search iteration %d\" % (ps_it+1)\n",
    "    algorithms.mc_pilco(\n",
    "        x0, forward_fn, dyn, pol, H, opt2, exp, 1000,\n",
    "        maximize=False, pegasus=True, mm_states=True,\n",
    "        mm_rewards=True, mpc=False, max_steps=25)\n",
    "    utils.plot_rollout(x0, forward_fn, pol, H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
